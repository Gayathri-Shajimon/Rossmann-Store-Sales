# -*- coding: utf-8 -*-
"""2201829_Gayathri Mol Shajimon_CE889_Group_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14WQMxh_lsYgTdyMwIZVyHMoZPeAI5xVB
"""

# import the required libraries

import numpy as np
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import seaborn as sns
import tensorflow as tf
from datetime import datetime
import os

# import the csv data into respective variables

stores_df = pd.read_csv('data/store.csv')
sales_df = pd.read_csv('data/train.csv', low_memory=False)
test_df = pd.read_csv('data/test.csv', low_memory=False)

# check test data shape

test_df.shape

# count sales data null values

sales_df.isnull().sum()

# count stores data null values

stores_df.isnull().sum()

# count test data null values

test_df.isnull().sum()

#Define Date column by years months and days:
sales_df['year'] = pd.DatetimeIndex(sales_df['Date']).year
sales_df['day'] = pd.DatetimeIndex(sales_df['Date']).day
sales_df['month'] = pd.DatetimeIndex(sales_df['Date']).month

#bar plot of sales by months of every year:
sns.barplot(data=sales_df, y='Sales', x=sales_df['year'], hue=sales_df['month'])

#bar plot of sales by days of every month:
sns.barplot(data=sales_df, y='Sales', x=sales_df['day'])

# plot the sales data to detect outliers

sales_df.boxplot( column =['Sales'], grid = False)

#function of outliers and the count:
##first and third quartile:
first_quartile =np.quantile(sales_df["Sales"], 0.25)
third_quartile = np.quantile(sales_df["Sales"], 0.75)
inter_quartile = third_quartile-first_quartile
upper=third_quartile+(1.5*inter_quartile)
lower=first_quartile-(1.5*inter_quartile)

outliers = sales_df["Sales"][(sales_df["Sales"] >= upper)]

print(round(upper),round(lower))
print(outliers)

# get the outliers which values are greater than the upper bound values

sales_df['outliers']=sales_df["Sales"][(sales_df["Sales"] >= upper)]

# plot the outliers data for each month, shows that december has most outliers because of holiday season

sales_table = sales_df[['month' , 'outliers' ]]
sales_table2 = sales_table[sales_table.outliers.notnull()]
monthlysales = sales_table2.month.value_counts()
month_df = monthlysales.to_frame()
month_df.sort_index(axis=0)
x = month_df.index 
y = month_df['month']
plt.bar(x, y) 
plt.xlabel('Months')
plt.ylabel('Outliers') 
plt.show()

# plot the outliers data for each year

import math
import matplotlib

sales_table = sales_df[['year' , 'outliers' ]]

# Removing null values
sales_table2 = sales_table[sales_table.outliers.notnull()]

# Taking count of distict values
yearly_sales = sales_table2.year.value_counts()
yearly_df = yearly_sales.to_frame()

#Ploting graph
f, ax = plt.subplots(figsize=(5,5)) 
x = yearly_df.index 
y = yearly_df['year']
plt.bar(x, y) 
plt.xlabel('Year')
plt.ylabel('Outliers')
x_interval = range(min(x), math.ceil(max(x))+1)
matplotlib.pyplot.xticks(x_interval)
plt.show()

# plot the outliers data for each data shows 2014 has most outliers

import matplotlib.dates as mdates
sales_table = sales_df[['Date' , 'outliers' ]]
sales_table2 = sales_table[sales_table.outliers.notnull()]
yearlysales = sales_table2.Date.value_counts()
year_df = yearlysales.to_frame()
year_df.sort_index(inplace=True)
x = year_df.index 
y = year_df['Date'] 

#Ploting the graph for yearly sale datewise
fig, ax = plt.subplots(figsize=(7, 6))

#setting interval to 4 months
year_locator = mdates.MonthLocator(interval=4)

# locating major locator are year
ax.xaxis.set_major_locator(year_locator)
plt.xlabel('Dates')
plt.ylabel('Outliers')

#Ploting the graph uisng axes subplot
ax.plot(x, y);
fig.autofmt_xdate()

# Removed columns that were created for the purpose of plotting grapg and analysis

sales_df = sales_df.drop(["outliers"], axis = 1) 
sales_df = sales_df.drop(["month"], axis = 1)
sales_df = sales_df.drop(["day"], axis = 1)
sales_df = sales_df.drop(["year"], axis = 1)

stores_df.head()

test_df.head()

# plot the outleirs data to finally remove them or cap them to a upper bound values, it shows mostly outliers are crossing the upper bound only

sales_df.boxplot( column =['Sales'], grid = False)

#cap values below low to low
sales_df.loc[sales_df['Sales'] < lower, 'Sales'] = lower

# #cap values above high to high
sales_df.loc[sales_df['Sales'] > upper, 'Sales'] = upper

sales_df.boxplot( column =['Sales'], grid = False)

# get the unique values for promointerval

stores_df['PromoInterval'].unique()

# map the unique values for promointerval to integer count

stores_df['PromoInterval'] = stores_df['PromoInterval'].map({'Jan,Apr,Jul,Oct': 4,'Feb,May,Aug,Nov': 4, 'Mar,Jun,Sept,Dec': 4})

# fill the null values for promointerval with 0

stores_df['PromoInterval'] = stores_df['PromoInterval'].fillna(0)

# merge promo2 since week and promo2 since year to one attribute promo2 since month to check if thats useful attribute

stores_df['Promo2SinceMonth'] = np.array((12-((stores_df['Promo2SinceWeek'])/4)) + ((2022 - (stores_df['Promo2SinceYear'])) * 12))
stores_df = stores_df.drop('Promo2SinceWeek', axis = 1)
stores_df = stores_df.drop('Promo2SinceYear', axis = 1)

sales_df.head()

# get the info for sales df

sales_df.info()

sales_df['Open']

# only take data for stores which are open

sales_df = sales_df[sales_df['Open'] == 1]

# get the info for sales df

test_df.info()

# get the unique values for test df

test_df['Open'].unique()

# fill the null values with 0

test_df['Open'] = test_df['Open'].fillna(0)

# fill competition distance with the median values

stores_df['CompetitionDistance'] = stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median())

# merge CompetitionOpenSinceMonth and CompetitionOpenSinceYear to one attribute CompetitionOpenSinceYear to check if thats useful attribute

stores_df['CompetitionOpenSinceTodayMonths'] = np.array((12 - (stores_df['CompetitionOpenSinceMonth'])) + ((2022 - (stores_df['CompetitionOpenSinceYear'])) * 12))
stores_df = stores_df.drop('CompetitionOpenSinceMonth', axis = 1)
stores_df = stores_df.drop('CompetitionOpenSinceYear', axis = 1)

# fill CompetitionOpenSinceTodayMonths with median

stores_df['CompetitionOpenSinceTodayMonths'] = stores_df['CompetitionOpenSinceTodayMonths'].fillna(stores_df['CompetitionOpenSinceTodayMonths'].median())

# join stores data with train df to map store values in train data 

train_df = sales_df.merge(stores_df, how='left', on="Store", validate="many_to_one")

# join stores data with test df to map store values in test data 

test_df_merged = test_df.merge(stores_df, how='left', on="Store", validate="many_to_one")

train_df.head()

test_df_merged.head()

# get unique values for mapping
train_df['StateHoliday'].unique()

# get unique values for mapping
train_df['Assortment'].unique()

# get unique values for mapping
train_df['StoreType'].unique()

# map unique values with one hot encoder for neural network
train_df['StateHoliday'] = train_df['StateHoliday'].map({'0': 0,'a': 1, 'b': 2,'c': 3})
train_df['Assortment'] = train_df['Assortment'].map({'a': 0, 'b': 1,'c': 2})
train_df['StoreType'] = train_df['StoreType'].map({'a': 0, 'b': 1,'c': 2, 'd': 3})

# map unique values with one hot encoder for neural network
test_df_merged['StateHoliday'] = test_df_merged['StateHoliday'].map({'0': 0,'a': 1, 'b': 2,'c': 3})
test_df_merged['Assortment'] = test_df_merged['Assortment'].map({'a': 0, 'b': 1,'c': 2})
test_df_merged['StoreType'] = test_df_merged['StoreType'].map({'a': 0, 'b': 1,'c': 2, 'd': 3})

# convert date to timestamp to convert the string to int
train_df['Date'] = pd.to_datetime(train_df['Date']).apply(lambda x: x.timestamp())
test_df_merged['Date'] = pd.to_datetime(test_df_merged['Date']).apply(lambda x: x.timestamp())

# get unique values for stateholiday
train_df['StateHoliday'].unique()

train_df.info()

train_df.DayOfWeek.unique()

# Commented out IPython magic to ensure Python compatibility.
# plot the corelation matrix to check which columns to keep and which to drop
import numpy as np 
from pandas import DataFrame
import seaborn as sns
# %matplotlib inline


corr = train_df.corr()

plt.figure(figsize = (15,8))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, center=0,  annot=True)
plt.show()

# choose only those column selected from corelation matrix
train_df = train_df.loc[:, ['StateHoliday', 'Assortment', 'Promo','DayOfWeek', 'Sales', 'Promo2', 'PromoInterval']]

# choose only those column selected from corelation matrix
test_df_merged = test_df_merged.loc[:, ['StateHoliday','Assortment', 'Promo',  'DayOfWeek', 'Promo2', 'PromoInterval']]

# neural network with keras
from numpy import loadtxt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# seperate target and train data
y = train_df['Sales']
train_df = train_df.drop('Sales', axis = 1)

train_df.head()

# select the max and min values for normalization
train_df_max = train_df.max()
train_df_min = train_df.min()
y_min = y.min()
y_max = y.max()

# apply min max normalization
train_df = (train_df - train_df.min()) / (train_df.max() - train_df.min())
y = (y - y.min()) / (y.max() - y.min())
test_df_merged = (test_df_merged - train_df.min()) / (train_df.max() - train_df.min())

train_df.head()

train_df.shape

# define the keras model with three layers
model = Sequential()
model.add(Dense(8, input_shape=(train_df.shape[1],), activation='relu'))
#model.add(Dense(6, activation='relu'))
model.add(Dense(3, activation='relu'))
model.add(Dense(1, activation = 'linear'))

# compile the keras model with rms prop as optimizer and MSE as error function
model.compile(loss='MeanSquaredError', optimizer='RMSprop', metrics=['mean_absolute_error'])

# fit the keras model on the dataset with 6 epochs and batches of 512 
history = model.fit(train_df, y, epochs=5, batch_size=512, validation_split = 0.1, shuffle= True)

# plot and the final graph of MSE loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
plt.savefig('training_group2.png')

# predict the test data
test_y = model.predict(test_df_merged)

test_df_merged.head()

train_df.head()

# denormalize the test data
test_y = test_y*(y_max-y_min) + y_min

# absolute any negative value if there in test data
test_y = abs(test_y)

# get the ids for test data
z = [int(i+1) for i in range(len(test_y))]

# create df for test data for submission
test_df = pd.DataFrame({'Id': z, 'Sales': np.transpose(test_y)[0]})

# create test data submission file
test_df.to_csv('Submission4.csv', sep=',', encoding='utf-8', index=False)

# save the model architecture
# model.save('4_model_nn_group_project')

# # save the model weights and architecture
# model_json = model.to_json()
# with open("4_model_nn_group_project.json", "w") as json_file:
#     json_file.write(model_json)
# # serialize weights to HDF5
# model.save_weights("4_model_nn_group_project.h5")
# print("Saved model to disk")

# # read the model weights and architecture

# from tensorflow.keras.models import Sequential, model_from_json
# json_file = open('4_model_nn_group_project.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)
# # load weights into new model
# loaded_model.load_weights("4_model_nn_group_project.h5")
# print("Loaded model from disk")